description: |
  **Data**
  Source of the data: Chronologer was trained using a combined database of 2.2 million peptide retention measurements from 11 large datasets that span diverse types of peptide modifications. Individual peptide libraries were aligned to a common reference space using Prosit-based predictions, and re-scaled to the hydrophobic index (% acetonitrile) using data from Khokin et al. for training SSRCalc.
  Type of molecules: Peptides
  Data acquisition: All peptides were analyzed by C18 RP-HPLC with an acetonitrile gradient in 0.1% formic acid
  False positives/negatives: The training data was filtered for 1% FDR. To address this issue, Chronologer was trained using a custom likelihood-based loss function that dynamically masks contributions to the gradient from observations that fall outside an inferred confidence interval based on a user-defined FDR. The loss function includes a learnable scale parameter per dataset to maintain a running estimate of the error from each data source to avoid over- or under-masking of data from datasets with less or more error, respectively.  
  **Optimization**
  Optimization target: Peptide retention time
  Experimental variability: Chronologer errors are comparable to expected column-to-column variation of peptide retention times
  Metric: Mean absolute error (MAE) was the primary evaluation metric, with the model trained using the FDR-masked negative log-likelihood function that filters likely false positive observations. 
  **Model**
  Interpretability: The model is a black box.
  Model type: The model performs regression, predicting a continuous RT value.
  Limitations of the model: The model is limited C18 RP-HPLC
  **Evaluation**
  Performance measures: MAE on datasets spanning multiple proteolytic enzymes, column temperature, and PTM-specific datasets
  Choice of performance measures: The MAE is the spread parameter for the Laplace distribution, which we experimentally find better models the distribution of chromatographic errors compared to MSE (which implicitly assumes a Gaussian distribution). A diverse set of datasets were chosen to evaluate the performance of the model under different experimental contexts.
  Comparison to SOTA: The model was compared with SSRCalc as well as Prosit using Skyline/mProphet.
  Evaluation method: The model was evaluated using Independent datasets.
  Model performance variability: Not highly variable under the tested conditions.
  **Input notes**
  **Output notes**
citation: |
  Deep learning from harmonized peptide libraries enables retention time prediction of diverse post translational modifications
  Wilburn, D.B., Shannon, A.E., Spicer, V., Richards, A.L., Yeung, D., Swaney, D.L., Krokhin, O.V., and Searle, B.C.
  bioRxiv 2023.05.30.542978;  https://doi.org/10.1101/2023.05.30.542978
tag: "Retention Time"
examples:
  inputs:
    [
      {
        "name": "peptide_sequences",
        "httpdtype": "BYTES",
        "data": '["AAAAAKAKM[UNIMOD:35]"]',
        "shape": "[1,1]"
      }
    ]